use std::{
    sync::Arc,
    time::{Duration, SystemTime},
};

use anyhow::Context;
#[cfg(feature = "prod-bin")]
use fasthash::{BufHasher, HasherExt, MetroHasherExt};
use spacetimedb_sdk::{Status, Timestamp};
use tokio::sync::broadcast;
use yral_spacetime_bindings::autogenerated::dedup_index::{self, add};

use crate::consts::{DEDUP_INDEX_MODULE_IDENTITY, STDB_ACCESS_TOKEN, STDB_URL};

pub type ReducerResult = Result<(), String>;

/// (input hash, reducer result)
type BusMessage = (u128, ReducerResult);

#[cfg(feature = "prod-bin")]
fn fast_hash<H: std::hash::Hash>(data: H) -> u128 {
    // set constant seed to get consistent result
    let mut hasher = MetroHasherExt::with_capacity_and_seed(0, Some(0));

    data.hash(&mut hasher);

    hasher.finish_ext()
}

#[cfg(not(feature = "prod-bin"))]
fn fast_hash<H: std::hash::Hash>(data: H) -> u128 {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};
    
    let mut hasher = DefaultHasher::new();
    data.hash(&mut hasher);
    hasher.finish() as u128
}

/// A wrapper around the [`dedup_index::DbConnection`] with an internal message bus that allows for async operations
#[derive(Clone)]
pub struct WrappedContext {
    pub conn: Arc<dedup_index::DbConnection>,
    tx: broadcast::Sender<BusMessage>,
}

impl WrappedContext {
    pub fn new() -> anyhow::Result<Self> {
        let conn = dedup_index::DbConnection::builder()
            .with_uri(STDB_URL)
            .with_module_name(DEDUP_INDEX_MODULE_IDENTITY)
            .with_token(Some(STDB_ACCESS_TOKEN.as_str()))
            .build()
            .context("Couldn't connect to the db")?;

        let conn = Arc::new(conn);
        let conn_for_ticking = conn.clone();

        tokio::spawn(async move {
            let Err(err) = conn_for_ticking.run_async().await else {
                return;
            };

            log::error!("connection to dedup index broke with an error: {err:#?}");
        });

        // this limit is for lagging mechanism of broadcast channel
        //
        // in our case, the receivers don't any slow work after receiving
        // messages, so we wont run into "slow receiver" problem.
        //
        // however, in case we end up with more than this limit number of
        // requests at the same time, the receiver would fail with error and
        // will most likely be retried by qstash
        let (tx, _) = broadcast::channel(65536);
        let tx_clone = tx.clone();

        conn.reducers
            .on_add(move |event, hash, video_id, timestamp| {
                let search_hash = fast_hash(HashData {
                    video_id: video_id.clone(),
                    hash: hash.clone(),
                    timestamp: *timestamp,
                });

                let res = match event.event.status {
                    Status::Committed => Ok(()),
                    Status::Failed(ref msg) => Err(msg.to_string()),
                    Status::OutOfEnergy => Err("Out of energy".into()),
                };

                // channel must be not be closed
                tx_clone.send((search_hash, res)).unwrap();
            });

        Ok(Self { conn, tx })
    }

    /// Adds a video hash to dedup index
    ///
    /// Outer error is any error when sending request, for example, network error.
    /// The inner error is the result of the operation itself
    ///
    /// taking ownership as that will require caller to clone and we need the underlying rx to be cloned to add a listener
    pub async fn add(
        &self,
        video_id: &str,
        hash: &str,
        timestamp: SystemTime,
    ) -> anyhow::Result<ReducerResult> {
        let search_hash = fast_hash(HashData {
            video_id: video_id.to_string(),
            hash: hash.to_string(),
            timestamp: timestamp.into(),
        });
        let mut rx = self.tx.subscribe();
        self.conn
            .reducers
            .add(hash.to_string(), video_id.to_string(), timestamp.into())
            .context("Couldn't send request to add")?;

        let res = loop {
            let (recv_hash, data) = tokio::time::timeout(Duration::from_secs(5), rx.recv())
                .await
                .context("timeout reached before receiving result")??;
            if recv_hash == search_hash {
                break data;
            }
        };

        Ok(res)
    }
}

#[derive(Debug, Hash)]
struct HashData {
    video_id: String,
    hash: String,
    timestamp: Timestamp,
}
