use std::time::{SystemTime, UNIX_EPOCH};

use crate::ai_video_detector::{AiVideoDetectorClient, Verdict};
use crate::events::types::string_or_number;
use crate::kvrocks::{
    BotUploadedAiContent, KvrocksClient, UserUploadedContentApproval,
    VideoMetadata as KvrocksVideoMetadata, VideoUniqueV2, VideohashOriginal, VideohashPhash,
};
use crate::{
    app_state,
    consts::{get_storj_video_url, DEDUP_INDEX_CANISTER_ID},
    duplicate_video::phash::{compute_phash_from_storj, VideoMetadata},
};
use anyhow::Context;
use google_cloud_bigquery::http::job::query::QueryRequest;
use google_cloud_bigquery::http::tabledata::insert_all::{InsertAllRequest, Row};
use redis::AsyncCommands;
use serde::{Deserialize, Serialize};
use serde_json::json;
use yral_canisters_client::dedup_index::{DedupIndex, SystemTime as CanisterSystemTime};

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct VideoPublisherDataV2 {
    pub publisher_principal: String,
    #[serde(deserialize_with = "string_or_number")]
    pub post_id: String, // Changed from u64 to String
}

/// The VideoHashDuplication struct will contain the deduplication logic
pub struct VideoHashDuplication;

impl VideoHashDuplication {
    #[allow(dead_code)]
    #[allow(clippy::too_many_arguments)]
    pub async fn process_video_deduplication<'a>(
        &self,
        agent: &ic_agent::Agent,
        bigquery_client: &google_cloud_bigquery::client::Client,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        _video_url: &str,
        publisher_data: VideoPublisherDataV2,
        publish_video_callback: impl FnOnce(
            &str,
            String, // Changed from u64 to &str
            String,
            &str,
        )
            -> futures::future::BoxFuture<'a, Result<(), anyhow::Error>>,
    ) -> Result<(), anyhow::Error> {
        log::info!("Computing phash for video ID: {video_id}");
        let (phash, metadata) =
            compute_phash_from_storj(&publisher_data.publisher_principal, video_id)
                .await
                .map_err(|e| anyhow::anyhow!("Failed to compute phash: {}", e))?;

        let is_duplicate = DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent)
            .is_duplicate(phash.clone())
            .await
            .context("Couldn't check if the video is duplicate")?;

        if is_duplicate {
            log::info!(
                "Duplicate video detected: hash: {} | video_id: {video_id}",
                phash
            );
        }

        // Store the phash regardless of duplication status
        self.store_videohash_to_dedup_index(agent, video_id, &phash)
            .await?;
        self.store_videohash_original(bigquery_client, kvrocks_client, video_id, &phash)
            .await?;
        self.store_phash_to_bigquery(bigquery_client, kvrocks_client, video_id, &phash, &metadata)
            .await?;
        self.store_user_uploaded_content_approval(
            bigquery_client,
            kvrocks_client,
            video_id,
            &publisher_data.post_id,
            &publisher_data.publisher_principal,
        )
        .await?;

        if !is_duplicate {
            self.store_unique_video_v2(kvrocks_client, video_id, &phash)
                .await?;
            log::info!("Unique video recorded: video_id [{video_id}]");
        }

        // Always proceed with normal video processing, regardless of duplicate status
        // because the deletion flow requires the embedding generated by the following steps in the pipeline
        //
        // TODO: once the deletion flow is restructured, stop the pipeline early
        let timestamp = chrono::Utc::now().to_rfc3339();
        publish_video_callback(
            video_id,
            publisher_data.post_id,
            timestamp,
            &publisher_data.publisher_principal,
        )
        .await?;

        Ok(())
    }

    async fn store_videohash_original(
        &self,
        bigquery_client: &google_cloud_bigquery::client::Client,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        hash: &str,
    ) -> Result<(), anyhow::Error> {
        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.videohash_original`
             (video_id, videohash, created_at)
             VALUES ('{video_id}', '{hash}', CURRENT_TIMESTAMP())"
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        log::info!("Storing hash in videohash_original for video_id [{video_id}]");

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        // Also push to kvrocks
        let hash_data = VideohashOriginal {
            video_id: video_id.to_string(),
            videohash: hash.to_string(),
            created_at: chrono::Utc::now().to_rfc3339(),
        };
        if let Err(e) = kvrocks_client.store_videohash_original(&hash_data).await {
            log::error!("Error pushing videohash_original to kvrocks: {}", e);
        }

        Ok(())
    }

    async fn store_phash_to_bigquery(
        &self,
        bigquery_client: &google_cloud_bigquery::client::Client,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        phash: &str,
        metadata: &VideoMetadata,
    ) -> Result<(), anyhow::Error> {
        log::info!(
            "Storing phash via streaming insert for video_id: {}",
            video_id
        );

        // Prepare row data
        let row_data = json!({
            "video_id": video_id,
            "phash": phash,
            "num_frames": 10,
            "hash_size": 8,
            "duration": metadata.duration,
            "width": metadata.width as i64,
            "height": metadata.height as i64,
            "fps": metadata.fps,
            "created_at": chrono::Utc::now().to_rfc3339(),
        });

        let request = InsertAllRequest {
            rows: vec![Row {
                insert_id: Some(format!(
                    "phash_dedup_{}_{}",
                    video_id,
                    chrono::Utc::now().timestamp_millis()
                )),
                json: row_data.clone(),
            }],
            ignore_unknown_values: Some(false),
            skip_invalid_rows: Some(false),
            ..Default::default()
        };

        let result = bigquery_client
            .tabledata()
            .insert(
                "hot-or-not-feed-intelligence",
                "yral_ds",
                "videohash_phash",
                &request,
            )
            .await?;

        // Check for insert errors
        if let Some(errors) = result.insert_errors {
            if !errors.is_empty() {
                log::error!("BigQuery streaming insert errors: {:?}", errors);
                anyhow::bail!("Failed to insert phash row: {:?}", errors);
            }
        }

        log::debug!("Successfully inserted phash for video_id: {}", video_id);

        // Also push to kvrocks
        let phash_data = VideohashPhash {
            video_id: video_id.to_string(),
            phash: phash.to_string(),
            num_frames: 10,
            hash_size: 8,
            duration: metadata.duration,
            width: metadata.width as i64,
            height: metadata.height as i64,
            fps: metadata.fps,
            created_at: chrono::Utc::now().to_rfc3339(),
        };
        if let Err(e) = kvrocks_client.store_videohash_phash(&phash_data).await {
            log::error!("Error pushing phash to kvrocks: {}", e);
        }

        Ok(())
    }

    async fn store_videohash_to_dedup_index(
        &self,
        agent: &ic_agent::Agent,
        video_id: &str,
        hash: &str,
    ) -> anyhow::Result<()> {
        let dedup_index = DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent);
        let now = SystemTime::now();

        let now = now.duration_since(UNIX_EPOCH).unwrap();
        dedup_index
            .add_video_to_index(
                video_id.into(),
                (
                    hash.into(),
                    CanisterSystemTime {
                        nanos_since_epoch: now.subsec_nanos(),
                        secs_since_epoch: now.as_secs(),
                    },
                ),
            )
            .await
            .context("Couldn't add video to dedup index")?;
        Ok(())
    }

    async fn store_unique_video_v2(
        &self,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        hash: &str,
    ) -> Result<(), anyhow::Error> {
        let bigquery_client = app_state::init_bigquery_client().await;

        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.video_unique_v2`
             (video_id, videohash, created_at)
             VALUES ('{video_id}', '{hash}', CURRENT_TIMESTAMP())"
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        log::info!("Storing unique video in video_unique_v2 for video_id [{video_id}]");

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        // Also push to kvrocks
        let unique_data = VideoUniqueV2 {
            video_id: video_id.to_string(),
            videohash: hash.to_string(),
            created_at: chrono::Utc::now().to_rfc3339(),
        };
        if let Err(e) = kvrocks_client.store_video_unique_v2(&unique_data).await {
            log::error!("Error pushing video_unique_v2 to kvrocks: {}", e);
        }

        Ok(())
    }

    async fn store_user_uploaded_content_approval(
        &self,
        bigquery_client: &google_cloud_bigquery::client::Client,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        post_id: &str,
        user_id: &str,
    ) -> Result<(), anyhow::Error> {
        log::info!("Processing content approval for video_id: {}", video_id);

        // Query country, canister_id, and type_ext from the video_upload_successful event
        let event_query = format!(
            "SELECT JSON_EXTRACT_SCALAR(params, '$.country') AS country,
                    JSON_EXTRACT_SCALAR(params, '$.canister_id') AS canister_id,
                    JSON_EXTRACT_SCALAR(params, '$.event_data.type_ext') AS type_ext
             FROM `hot-or-not-feed-intelligence.analytics_335143420.test_events_analytics`
             WHERE event = 'video_upload_successful'
               AND JSON_EXTRACT_SCALAR(params, '$.video_id') = '{}'
             LIMIT 1",
            video_id
        );

        let event_request = QueryRequest {
            query: event_query,
            ..Default::default()
        };

        let event_result = bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &event_request)
            .await;

        let (is_bot, canister_id) = match event_result {
            Ok(response) => {
                if let Some(rows) = response.rows {
                    if let Some(row) = rows.first() {
                        let is_bot_country = row.f.first().is_some_and(|cell| match &cell.v {
                            google_cloud_bigquery::http::tabledata::list::Value::String(
                                country,
                            ) => {
                                let is_bot = country.ends_with("-BOT");
                                log::debug!(
                                    "Video {} has country '{}', is_bot: {}",
                                    video_id,
                                    country,
                                    is_bot
                                );
                                is_bot
                            }
                            _ => false,
                        });

                        let canister_id = row.f.get(1).and_then(|cell| match &cell.v {
                            google_cloud_bigquery::http::tabledata::list::Value::String(id) => {
                                Some(id.clone())
                            }
                            _ => None,
                        });

                        let is_ai_video = row.f.get(2).is_some_and(|cell| match &cell.v {
                            google_cloud_bigquery::http::tabledata::list::Value::String(
                                type_ext,
                            ) => {
                                let is_ai = type_ext == "ai_video";
                                log::debug!(
                                    "Video {} has type_ext '{}', is_ai_video: {}",
                                    video_id,
                                    type_ext,
                                    is_ai
                                );
                                is_ai
                            }
                            _ => false,
                        });

                        let is_bot = is_bot_country || is_ai_video;

                        (is_bot, canister_id)
                    } else {
                        log::debug!(
                            "No video_upload_successful event found for video {}",
                            video_id
                        );
                        (false, None)
                    }
                } else {
                    (false, None)
                }
            }
            Err(e) => {
                log::warn!("Failed to query event data for video {}: {}", video_id, e);
                (false, None)
            }
        };

        // Store video metadata for all videos
        let metadata = KvrocksVideoMetadata {
            video_id: video_id.to_string(),
            post_id: post_id.to_string(),
            publisher_user_id: user_id.to_string(),
        };
        if let Err(e) = kvrocks_client.store_video_metadata(&metadata).await {
            log::error!("Error storing video metadata to kvrocks: {}", e);
        }

        if is_bot {
            log::info!(
                "Bot video detected, storing bot marker for video: {}",
                video_id
            );
            // Store bot marker in kvrocks
            let bot_data = BotUploadedAiContent {
                video_id: video_id.to_string(),
                created_at: chrono::Utc::now().to_rfc3339(),
            };
            if let Err(e) = kvrocks_client
                .store_bot_uploaded_ai_content(&bot_data)
                .await
            {
                log::error!("Error storing bot marker to kvrocks: {}", e);
            }
            return Ok(());
        }

        // Run AI video detection FIRST to determine approval status
        let ai_detector = AiVideoDetectorClient::new();
        let video_url = get_storj_video_url(user_id, video_id, false);

        let is_approved = if ai_detector.is_configured() {
            log::info!("Running AI detection for video {}: {}", video_id, video_url);

            match ai_detector.detect_video(&video_url).await {
                Ok(response) => {
                    log::info!(
                        "AI detection result for video {}: verdict={:?}, confidence={:.2}",
                        video_id,
                        response.verdict,
                        response.confidence
                    );

                    match response.verdict {
                        Verdict::Allow => {
                            // AI-generated content - auto-approve
                            log::info!("Auto-approving AI-generated video: {}", video_id);
                            true
                        }
                        Verdict::Block => {
                            // Real footage - don't store, reject immediately
                            log::info!("Rejecting real footage video: {}", video_id);
                            return Ok(()); // Don't store this video at all
                        }
                        Verdict::Review => {
                            // Uncertain - needs manual review
                            log::info!(
                                "Video {} needs manual review (confidence: {:.2})",
                                video_id,
                                response.confidence
                            );
                            false
                        }
                    }
                }
                Err(e) => {
                    log::error!(
                        "AI detection failed for video {}: {}. Sending to manual review.",
                        video_id,
                        e
                    );
                    false // On error, send to manual review
                }
            }
        } else {
            log::warn!(
                "AI video detector not configured, sending video {} to manual review",
                video_id
            );
            false // Not configured, send to manual review
        };

        let approval_data = UserUploadedContentApproval {
            video_id: video_id.to_string(),
            post_id: post_id.to_string(),
            canister_id: canister_id.clone().unwrap_or_default(),
            user_id: user_id.to_string(),
            is_approved,
            created_at: chrono::Utc::now().to_rfc3339(),
        };
        if let Err(e) = kvrocks_client
            .store_user_uploaded_content_approval(&approval_data)
            .await
        {
            log::error!(
                "Error storing user_uploaded_content_approval to kvrocks: {}",
                e
            );
        }

        log::info!(
            "Stored ugc_content_approval to kvrocks: video_id={}, post_id={}, canister_id={:?}, user_id={}, is_approved={}",
            video_id,
            post_id,
            canister_id,
            user_id,
            is_approved
        );

        let video_id_owned = video_id.to_string();
        let post_id_owned = post_id.to_string();
        let user_id_owned = user_id.to_string();
        let canister_id_owned = canister_id.clone();
        let bigquery_client = bigquery_client.clone();
        // Use streaming for approved videos but for review use manual
        if is_approved {
            tokio::spawn(async move {
                let row_data = json!({
                    "video_id": video_id_owned,
                    "post_id": post_id_owned,
                    "canister_id": canister_id_owned.unwrap_or_default(),
                    "user_id": user_id_owned,
                    "is_approved": is_approved,
                    "created_at": chrono::Utc::now().to_rfc3339(),
                });

                let request = InsertAllRequest {
                    rows: vec![Row {
                        insert_id: Some(format!(
                            "ugc_approval_{}_{}",
                            video_id_owned,
                            chrono::Utc::now().timestamp_millis()
                        )),
                        json: row_data,
                    }],
                    ignore_unknown_values: Some(false),
                    skip_invalid_rows: Some(false),
                    ..Default::default()
                };

                match bigquery_client
                    .tabledata()
                    .insert(
                        "hot-or-not-feed-intelligence",
                        "yral_ds",
                        "ugc_content_approval",
                        &request,
                    )
                    .await
                {
                    Ok(result) => {
                        if let Some(errors) = result.insert_errors {
                            if !errors.is_empty() {
                                log::error!(
                                    "BigQuery streaming insert errors for video_id={}: {:?}",
                                    video_id_owned,
                                    errors
                                );
                            }
                        } else {
                            log::info!(
                                "Successfully inserted ugc_content_approval via streaming for video_id={}, is_approved={}",
                                video_id_owned,
                                is_approved
                            );
                        }
                    }
                    Err(e) => {
                        log::error!(
                            "Failed to insert ugc_content_approval to BigQuery for video_id={}: {}",
                            video_id_owned,
                            e
                        );
                    }
                }
            });
        } else {
            tokio::spawn(async move {
                let canister_id_sql = canister_id_owned
                    .as_ref()
                    .map(|id| format!("'{}'", id.replace('\'', "''")))
                    .unwrap_or_else(|| "NULL".to_string());

                let query = format!(
                    "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.ugc_content_approval`
                     (video_id, post_id, canister_id, user_id, is_approved, created_at)
                     VALUES ('{}', '{}', {}, '{}', {}, CURRENT_TIMESTAMP())",
                    video_id_owned.replace('\'', "''"),
                    post_id_owned.replace('\'', "''"),
                    canister_id_sql,
                    user_id_owned.replace('\'', "''"),
                    is_approved
                );

                let request = QueryRequest {
                    query,
                    ..Default::default()
                };

                if let Err(e) = bigquery_client
                    .job()
                    .query("hot-or-not-feed-intelligence", &request)
                    .await
                {
                    log::error!(
                        "Failed to insert ugc_content_approval to BigQuery for video_id={}: {}",
                        video_id_owned,
                        e
                    );
                } else {
                    log::info!(
                        "Successfully inserted ugc_content_approval to BigQuery: video_id={}, is_approved={}",
                        video_id_owned,
                        is_approved
                    );
                }
            });
        }

        Ok(())
    }

    /// Check Redis for exact phash match (tier-1 caching)
    async fn check_exact_duplicate_in_redis(
        dragonfly_pool: &std::sync::Arc<crate::yral_auth::dragonfly::DragonflyPool>,
        phash: &str,
    ) -> Result<Option<String>, anyhow::Error> {
        let mut conn = dragonfly_pool
            .get()
            .await
            .context("Failed to get Dragonfly connection")?;

        let key = format!("impressions:video_phash:{}", phash);
        let result: Option<String> = conn
            .get(&key)
            .await
            .context("Failed to query Redis for phash")?;

        Ok(result)
    }

    /// Store unique phash in Redis for fast exact match lookups
    async fn store_unique_phash_in_redis(
        dragonfly_pool: &std::sync::Arc<crate::yral_auth::dragonfly::DragonflyPool>,
        phash: &str,
        video_id: &str,
    ) -> Result<(), anyhow::Error> {
        let mut conn = dragonfly_pool
            .get()
            .await
            .context("Failed to get Dragonfly connection")?;

        let key = format!("impressions:video_phash:{}", phash);
        conn.set::<_, _, ()>(&key, video_id)
            .await
            .context("Failed to store phash in Redis")?;

        Ok(())
    }

    /// V2 version that uses Milvus for deduplication instead of DedupIndex canister
    /// Provides configurable hamming distance threshold, Redis tier-1 caching, and graceful fallback
    #[cfg(not(feature = "local-bin"))]
    #[allow(clippy::too_many_arguments)]
    pub async fn process_video_deduplication_v2<'a>(
        &self,
        agent: &ic_agent::Agent,
        bigquery_client: &google_cloud_bigquery::client::Client,
        milvus_client: &Option<crate::milvus::Client>,
        dragonfly_pool: &std::sync::Arc<crate::yral_auth::dragonfly::DragonflyPool>,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        _video_url: &str,
        publisher_data: VideoPublisherDataV2,
        hamming_threshold: u32,
        publish_video_callback: impl FnOnce(
            &str,
            String,
            String,
            &str,
        )
            -> futures::future::BoxFuture<'a, Result<(), anyhow::Error>>,
    ) -> Result<(), anyhow::Error> {
        log::info!(
            "Computing phash for video ID: {video_id} (v2 with Milvus, threshold={})",
            hamming_threshold
        );
        let (phash, metadata) =
            compute_phash_from_storj(&publisher_data.publisher_principal, video_id)
                .await
                .map_err(|e| anyhow::anyhow!("Failed to compute phash: {}", e))?;

        // TIER 1: Check Redis for exact match (FAST - <1ms)
        log::debug!("Tier 1: Checking Redis for exact phash match");
        if let Ok(Some(existing_video_id)) =
            Self::check_exact_duplicate_in_redis(dragonfly_pool, &phash).await
        {
            log::info!(
                "âš¡ EXACT DUPLICATE (Redis): Video {} has identical phash to {}",
                video_id,
                existing_video_id
            );

            // Store metadata (kvrocks push is inside the functions)
            self.store_videohash_original(bigquery_client, kvrocks_client, video_id, &phash)
                .await?;
            self.store_phash_to_bigquery(
                bigquery_client,
                kvrocks_client,
                video_id,
                &phash,
                &metadata,
            )
            .await?;
            self.store_user_uploaded_content_approval(
                bigquery_client,
                kvrocks_client,
                video_id,
                &publisher_data.post_id,
                &publisher_data.publisher_principal,
            )
            .await?;

            // Continue with video processing pipeline
            let timestamp = chrono::Utc::now().to_rfc3339();
            publish_video_callback(
                video_id,
                publisher_data.post_id,
                timestamp,
                &publisher_data.publisher_principal,
            )
            .await?;

            return Ok(());
        }

        log::debug!("Tier 1: No exact match in Redis");

        // TIER 2: Check Milvus for similar matches (SLOWER - 10-50ms)
        log::debug!(
            "Tier 2: Checking Milvus for similar videos (Hamming distance < {})",
            hamming_threshold
        );
        let is_duplicate = if let Some(client) = milvus_client {
            log::debug!(
                "Checking Milvus for duplicates with threshold {}",
                hamming_threshold
            );
            match crate::milvus::search_similar_videos(client, &phash, hamming_threshold).await {
                Ok(results) => {
                    let is_dup = !results.is_empty();
                    if is_dup {
                        log::info!(
                            "ðŸ” SIMILAR DUPLICATE (Milvus): Video {} matches {} videos",
                            video_id,
                            results.len()
                        );
                    }
                    is_dup
                }
                Err(e) => {
                    log::warn!(
                        "Milvus search failed ({}), falling back to DedupIndex: {}",
                        video_id,
                        e
                    );
                    // Fallback to DedupIndex canister
                    DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent)
                        .is_duplicate(phash.clone())
                        .await
                        .context("Couldn't check if the video is duplicate (fallback)")?
                }
            }
        } else {
            log::debug!("Milvus client not available, using DedupIndex canister");
            // No Milvus client, use DedupIndex canister
            DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent)
                .is_duplicate(phash.clone())
                .await
                .context("Couldn't check if the video is duplicate")?
        };

        // Store the phash regardless of duplication status (kvrocks push is inside the functions)
        self.store_videohash_original(bigquery_client, kvrocks_client, video_id, &phash)
            .await?;
        self.store_phash_to_bigquery(bigquery_client, kvrocks_client, video_id, &phash, &metadata)
            .await?;
        self.store_user_uploaded_content_approval(
            bigquery_client,
            kvrocks_client,
            video_id,
            &publisher_data.post_id,
            &publisher_data.publisher_principal,
        )
        .await?;

        if !is_duplicate {
            log::info!("âœ¨ UNIQUE: Video {} has a unique phash", video_id);

            // Insert into Milvus + Redis (only for unique videos)
            if let Some(client) = milvus_client {
                let created_at = chrono::Utc::now().timestamp();

                // Insert into Milvus
                log::debug!("Inserting unique video into Milvus: {}", video_id);
                if let Err(e) =
                    crate::milvus::insert_video_hash(client, video_id, &phash, created_at).await
                {
                    log::error!("Failed to insert video {} into Milvus: {}", video_id, e);
                }

                // Insert into Redis for fast exact match caching
                if let Err(e) =
                    Self::store_unique_phash_in_redis(dragonfly_pool, &phash, video_id).await
                {
                    log::error!("Failed to insert video {} into Redis: {}", video_id, e);
                }
            }

            self.store_unique_video_v2(kvrocks_client, video_id, &phash)
                .await?;
            log::info!("Unique video recorded in BigQuery: video_id [{video_id}]");
        } else {
            log::debug!(
                "Video {} is a duplicate, NOT storing in Milvus/Redis",
                video_id
            );
        }

        // Always proceed with normal video processing, regardless of duplicate status
        let timestamp = chrono::Utc::now().to_rfc3339();
        publish_video_callback(
            video_id,
            publisher_data.post_id,
            timestamp,
            &publisher_data.publisher_principal,
        )
        .await?;

        Ok(())
    }
}
