use std::time::{SystemTime, UNIX_EPOCH};

use crate::{app_state, consts::DEDUP_INDEX_CANISTER_ID, duplicate_video::videohash::VideoHash};
use anyhow::Context;
use google_cloud_bigquery::http::job::query::QueryRequest;
use serde::{Deserialize, Serialize};
use yral_canisters_client::dedup_index::{DedupIndex, SystemTime as CanisterSystemTime};

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct VideoPublisherData {
    pub publisher_principal: String,
    pub post_id: u64,
}

// Add these structures to support the indexer API response
#[derive(Debug, Deserialize)]
struct VideoHashIndexerResponse {
    match_found: bool,
    match_details: Option<MatchDetails>,
    hash_added: bool,
}

#[derive(Debug, Deserialize)]
struct MatchDetails {
    video_id: String,
    similarity_percentage: f64,
    is_duplicate: bool,
}

// The VideoHashDuplication struct will contain the deduplication logic
pub struct VideoHashDuplication<'a> {
    client: &'a reqwest::Client,
    base_url: &'a reqwest::Url,
}

impl<'a> VideoHashDuplication<'a> {
    pub fn new(client: &'a reqwest::Client, base_url: &'a reqwest::Url) -> Self {
        Self { client, base_url }
    }

    pub async fn process_video_deduplication(
        &self,
        agent: &ic_agent::Agent,
        bigquery_client: &google_cloud_bigquery::client::Client,
        video_id: &str,
        video_url: &str,
        publisher_data: VideoPublisherData,
        publish_video_callback: impl FnOnce(
            &str,
            u64,
            String,
            &str,
        )
            -> futures::future::BoxFuture<'a, Result<(), anyhow::Error>>,
    ) -> Result<(), anyhow::Error> {
        log::info!("Calculating videohash for video URL: {}", video_url);
        let video_hash = VideoHash::from_url(video_url)
            .await
            .map_err(|e| anyhow::anyhow!("Failed to generate videohash: {}", e))?;

        let is_duplicate = DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent)
            .is_duplicate(video_hash.hash.clone())
            .await
            .context("Couldn't check if the video is duplicate")?;

        if is_duplicate {
            log::info!(
                "Duplicate video detected: hash: {} | video_id: {video_id}",
                video_hash.hash
            );
        }

        // Store the original hash regardless of duplication status
        self.store_videohash_to_dedup_index(agent, video_id, &video_hash.hash)
            .await?;
        self.store_videohash_original(bigquery_client, video_id, &video_hash.hash)
            .await?;

        if !is_duplicate {
            self.store_unique_video(video_id, &video_hash.hash).await?;
            self.store_unique_video_v2(video_id, &video_hash.hash)
                .await?;
            log::info!("Unique video recorded: video_id [{}]", video_id);
        }

        let guard = sentry::Hub::current().push_scope();
        sentry::configure_scope(|scope| {
            scope.set_tag("yral.video_id", &video_id);
            scope.set_tag(
                "yral.publisher_user_id",
                &publisher_data.publisher_principal,
            );
            scope.set_tag("yral.is_duplicate", is_duplicate);
            scope.set_tag("yral.video_hash", &video_hash.hash);
        });
        sentry::capture_message("finished deduplication step", sentry::Level::Info);
        drop(guard);

        // Always proceed with normal video processing, regardless of duplicate status
        // because the deletion flow requires the embedding generated by the following steps in the pipeline
        //
        // TODO: once the deletion flow is restructured, stop the pipeline early
        let timestamp = chrono::Utc::now().to_rfc3339();
        publish_video_callback(
            video_id,
            publisher_data.post_id,
            timestamp,
            &publisher_data.publisher_principal,
        )
        .await?;

        Ok(())
    }

    async fn store_videohash_original(
        &self,
        bigquery_client: &google_cloud_bigquery::client::Client,
        video_id: &str,
        hash: &str,
    ) -> Result<(), anyhow::Error> {
        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.videohash_original` 
             (video_id, videohash, created_at) 
             VALUES ('{}', '{}', CURRENT_TIMESTAMP())",
            video_id, hash
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        log::info!(
            "Storing hash in videohash_original for video_id [{}]",
            video_id
        );

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        Ok(())
    }

    async fn store_videohash_to_dedup_index(
        &self,
        agent: &ic_agent::Agent,
        video_id: &str,
        hash: &str,
    ) -> anyhow::Result<()> {
        let dedup_index = DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent);
        let now = SystemTime::now();

        let now = now.duration_since(UNIX_EPOCH).unwrap();
        dedup_index
            .add_video_to_index(
                video_id.into(),
                (
                    hash.into(),
                    CanisterSystemTime {
                        nanos_since_epoch: now.subsec_nanos(),
                        secs_since_epoch: now.as_secs(),
                    },
                ),
            )
            .await
            .context("Couldn't add video to dedup index")?;
        Ok(())
    }

    async fn store_unique_video(&self, video_id: &str, hash: &str) -> Result<(), anyhow::Error> {
        let bigquery_client = app_state::init_bigquery_client().await;

        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.video_unique` 
             (video_id, videohash, created_at) 
             VALUES ('{}', '{}', CURRENT_TIMESTAMP())",
            video_id, hash
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        log::info!(
            "Storing unique video in video_unique for video_id [{}]",
            video_id
        );

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        Ok(())
    }

    async fn store_unique_video_v2(&self, video_id: &str, hash: &str) -> Result<(), anyhow::Error> {
        let bigquery_client = app_state::init_bigquery_client().await;

        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.video_unique_v2` 
             (video_id, videohash, created_at) 
             VALUES ('{}', '{}', CURRENT_TIMESTAMP())",
            video_id, hash
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        log::info!(
            "Storing unique video in video_unique for video_id [{}]",
            video_id
        );

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        Ok(())
    }
}
