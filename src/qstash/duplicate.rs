use std::time::{SystemTime, UNIX_EPOCH};

use crate::events::types::string_or_number;
use crate::{
    app_state,
    consts::DEDUP_INDEX_CANISTER_ID,
    duplicate_video::phash::{compute_phash_from_cloudflare, VideoMetadata},
};
use anyhow::Context;
use google_cloud_bigquery::http::job::query::QueryRequest;
use google_cloud_bigquery::http::tabledata::insert_all::{InsertAllRequest, Row};
use serde::{Deserialize, Serialize};
use serde_json::json;
use yral_canisters_client::dedup_index::{DedupIndex, SystemTime as CanisterSystemTime};

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct VideoPublisherDataV2 {
    pub publisher_principal: String,
    #[serde(deserialize_with = "string_or_number")]
    pub post_id: String, // Changed from u64 to String
}

/// The VideoHashDuplication struct will contain the deduplication logic
pub struct VideoHashDuplication;

impl VideoHashDuplication {
    pub async fn process_video_deduplication<'a>(
        &self,
        agent: &ic_agent::Agent,
        bigquery_client: &google_cloud_bigquery::client::Client,
        video_id: &str,
        _video_url: &str,
        publisher_data: VideoPublisherDataV2,
        publish_video_callback: impl FnOnce(
            &str,
            String, // Changed from u64 to &str
            String,
            &str,
        )
            -> futures::future::BoxFuture<'a, Result<(), anyhow::Error>>,
    ) -> Result<(), anyhow::Error> {
        log::info!("Computing phash for video ID: {video_id}");
        let (phash, metadata) = compute_phash_from_cloudflare(video_id)
            .await
            .map_err(|e| anyhow::anyhow!("Failed to compute phash: {}", e))?;

        let is_duplicate = DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent)
            .is_duplicate(phash.clone())
            .await
            .context("Couldn't check if the video is duplicate")?;

        if is_duplicate {
            log::info!(
                "Duplicate video detected: hash: {} | video_id: {video_id}",
                phash
            );
        }

        // Store the phash regardless of duplication status
        self.store_videohash_to_dedup_index(agent, video_id, &phash)
            .await?;
        self.store_videohash_original(bigquery_client, video_id, &phash)
            .await?;
        self.store_phash_to_bigquery(bigquery_client, video_id, &phash, &metadata)
            .await?;

        if !is_duplicate {
            self.store_unique_video(video_id, &phash).await?;
            self.store_unique_video_v2(video_id, &phash).await?;
            log::info!("Unique video recorded: video_id [{video_id}]");
        }

        // Always proceed with normal video processing, regardless of duplicate status
        // because the deletion flow requires the embedding generated by the following steps in the pipeline
        //
        // TODO: once the deletion flow is restructured, stop the pipeline early
        let timestamp = chrono::Utc::now().to_rfc3339();
        publish_video_callback(
            video_id,
            publisher_data.post_id,
            timestamp,
            &publisher_data.publisher_principal,
        )
        .await?;

        Ok(())
    }

    async fn store_videohash_original(
        &self,
        bigquery_client: &google_cloud_bigquery::client::Client,
        video_id: &str,
        hash: &str,
    ) -> Result<(), anyhow::Error> {
        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.videohash_original`
             (video_id, videohash, created_at)
             VALUES ('{video_id}', '{hash}', CURRENT_TIMESTAMP())"
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        log::info!("Storing hash in videohash_original for video_id [{video_id}]");

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        Ok(())
    }

    async fn store_phash_to_bigquery(
        &self,
        bigquery_client: &google_cloud_bigquery::client::Client,
        video_id: &str,
        phash: &str,
        metadata: &VideoMetadata,
    ) -> Result<(), anyhow::Error> {
        log::info!(
            "Storing phash via streaming insert for video_id: {}",
            video_id
        );

        // Prepare row data
        let row_data = json!({
            "video_id": video_id,
            "phash": phash,
            "num_frames": 10,
            "hash_size": 8,
            "duration": metadata.duration,
            "width": metadata.width as i64,
            "height": metadata.height as i64,
            "fps": metadata.fps,
            "created_at": chrono::Utc::now().to_rfc3339(),
        });

        let request = InsertAllRequest {
            rows: vec![Row {
                insert_id: Some(format!(
                    "phash_dedup_{}_{}",
                    video_id,
                    chrono::Utc::now().timestamp_millis()
                )),
                json: row_data,
            }],
            ignore_unknown_values: Some(false),
            skip_invalid_rows: Some(false),
            ..Default::default()
        };

        let result = bigquery_client
            .tabledata()
            .insert(
                "hot-or-not-feed-intelligence",
                "yral_ds",
                "videohash_phash",
                &request,
            )
            .await?;

        // Check for insert errors
        if let Some(errors) = result.insert_errors {
            if !errors.is_empty() {
                log::error!("BigQuery streaming insert errors: {:?}", errors);
                anyhow::bail!("Failed to insert phash row: {:?}", errors);
            }
        }

        log::debug!("Successfully inserted phash for video_id: {}", video_id);
        Ok(())
    }

    async fn store_videohash_to_dedup_index(
        &self,
        agent: &ic_agent::Agent,
        video_id: &str,
        hash: &str,
    ) -> anyhow::Result<()> {
        let dedup_index = DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent);
        let now = SystemTime::now();

        let now = now.duration_since(UNIX_EPOCH).unwrap();
        dedup_index
            .add_video_to_index(
                video_id.into(),
                (
                    hash.into(),
                    CanisterSystemTime {
                        nanos_since_epoch: now.subsec_nanos(),
                        secs_since_epoch: now.as_secs(),
                    },
                ),
            )
            .await
            .context("Couldn't add video to dedup index")?;
        Ok(())
    }

    async fn store_unique_video(&self, video_id: &str, hash: &str) -> Result<(), anyhow::Error> {
        let bigquery_client = app_state::init_bigquery_client().await;

        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.video_unique` 
             (video_id, videohash, created_at) 
             VALUES ('{video_id}', '{hash}', CURRENT_TIMESTAMP())"
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        log::info!("Storing unique video in video_unique for video_id [{video_id}]");

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        Ok(())
    }

    async fn store_unique_video_v2(&self, video_id: &str, hash: &str) -> Result<(), anyhow::Error> {
        let bigquery_client = app_state::init_bigquery_client().await;

        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.video_unique_v2` 
             (video_id, videohash, created_at) 
             VALUES ('{video_id}', '{hash}', CURRENT_TIMESTAMP())"
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        log::info!("Storing unique video in video_unique for video_id [{video_id}]");

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        Ok(())
    }
}
