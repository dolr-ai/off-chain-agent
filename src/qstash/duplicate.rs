use std::time::{SystemTime, UNIX_EPOCH};

use crate::events::types::string_or_number;
use crate::kvrocks::{
    BotUploadedAiContent, KvrocksClient, UserUploadedContentApproval,
    VideoMetadata as KvrocksVideoMetadata, VideoUniqueV2, VideohashOriginal, VideohashPhash,
};
use crate::{
    app_state,
    consts::DEDUP_INDEX_CANISTER_ID,
    duplicate_video::phash::{compute_phash_from_storj, VideoMetadata},
};
use anyhow::Context;
use google_cloud_bigquery::http::job::query::QueryRequest;
use google_cloud_bigquery::http::tabledata::insert_all::{InsertAllRequest, Row};
use serde::{Deserialize, Serialize};
use serde_json::json;
use yral_canisters_client::dedup_index::{DedupIndex, SystemTime as CanisterSystemTime};

type RedisPool = bb8::Pool<bb8_redis::RedisConnectionManager>;

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct VideoPublisherDataV2 {
    pub publisher_principal: String,
    #[serde(deserialize_with = "string_or_number")]
    pub post_id: String, // Changed from u64 to String
}

/// The VideoHashDuplication struct will contain the deduplication logic
pub struct VideoHashDuplication;

impl VideoHashDuplication {
    #[allow(dead_code)]
    #[allow(clippy::too_many_arguments)]
    pub async fn process_video_deduplication<'a>(
        &self,
        agent: &ic_agent::Agent,
        bigquery_client: &google_cloud_bigquery::client::Client,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        _video_url: &str,
        publisher_data: VideoPublisherDataV2,
        publish_video_callback: impl FnOnce(
            &str,
            String, // Changed from u64 to &str
            String,
            &str,
        )
            -> futures::future::BoxFuture<'a, Result<(), anyhow::Error>>,
    ) -> Result<(), anyhow::Error> {
        log::info!("Computing phash for video ID: {video_id}");
        let (phash, metadata) =
            compute_phash_from_storj(&publisher_data.publisher_principal, video_id)
                .await
                .map_err(|e| anyhow::anyhow!("Failed to compute phash: {}", e))?;

        let is_duplicate = DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent)
            .is_duplicate(phash.clone())
            .await
            .context("Couldn't check if the video is duplicate")?;

        if is_duplicate {
            log::info!(
                "Duplicate video detected: hash: {} | video_id: {video_id}",
                phash
            );
        }

        // Store the phash regardless of duplication status
        self.store_videohash_to_dedup_index(agent, video_id, &phash)
            .await?;
        self.store_videohash_original(bigquery_client, kvrocks_client, video_id, &phash)
            .await?;
        self.store_phash_to_bigquery(bigquery_client, kvrocks_client, video_id, &phash, &metadata)
            .await?;
        self.store_user_uploaded_content_approval(
            bigquery_client,
            kvrocks_client,
            video_id,
            &publisher_data.post_id,
            &publisher_data.publisher_principal,
        )
        .await?;

        if !is_duplicate {
            self.store_unique_video_v2(kvrocks_client, video_id, &phash)
                .await?;
            log::info!("Unique video recorded: video_id [{video_id}]");
        }

        // Always proceed with normal video processing, regardless of duplicate status
        // because the deletion flow requires the embedding generated by the following steps in the pipeline
        //
        // TODO: once the deletion flow is restructured, stop the pipeline early
        let timestamp = chrono::Utc::now().to_rfc3339();
        publish_video_callback(
            video_id,
            publisher_data.post_id,
            timestamp,
            &publisher_data.publisher_principal,
        )
        .await?;

        Ok(())
    }

    async fn store_videohash_original(
        &self,
        bigquery_client: &google_cloud_bigquery::client::Client,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        hash: &str,
    ) -> Result<(), anyhow::Error> {
        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.videohash_original`
             (video_id, videohash, created_at)
             VALUES ('{video_id}', '{hash}', CURRENT_TIMESTAMP())"
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        log::info!("Storing hash in videohash_original for video_id [{video_id}]");

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        // Also push to kvrocks
        let hash_data = VideohashOriginal {
            video_id: video_id.to_string(),
            videohash: hash.to_string(),
            created_at: chrono::Utc::now().to_rfc3339(),
        };
        if let Err(e) = kvrocks_client.store_videohash_original(&hash_data).await {
            log::error!("Error pushing videohash_original to kvrocks: {}", e);
        }

        Ok(())
    }

    async fn store_phash_to_bigquery(
        &self,
        bigquery_client: &google_cloud_bigquery::client::Client,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        phash: &str,
        metadata: &VideoMetadata,
    ) -> Result<(), anyhow::Error> {
        log::info!(
            "Storing phash via streaming insert for video_id: {}",
            video_id
        );

        // Prepare row data
        let row_data = json!({
            "video_id": video_id,
            "phash": phash,
            "num_frames": 10,
            "hash_size": 8,
            "duration": metadata.duration,
            "width": metadata.width as i64,
            "height": metadata.height as i64,
            "fps": metadata.fps,
            "created_at": chrono::Utc::now().to_rfc3339(),
        });

        let request = InsertAllRequest {
            rows: vec![Row {
                insert_id: Some(format!(
                    "phash_dedup_{}_{}",
                    video_id,
                    chrono::Utc::now().timestamp_millis()
                )),
                json: row_data.clone(),
            }],
            ignore_unknown_values: Some(false),
            skip_invalid_rows: Some(false),
            ..Default::default()
        };

        let result = bigquery_client
            .tabledata()
            .insert(
                "hot-or-not-feed-intelligence",
                "yral_ds",
                "videohash_phash",
                &request,
            )
            .await?;

        // Check for insert errors
        if let Some(errors) = result.insert_errors {
            if !errors.is_empty() {
                log::error!("BigQuery streaming insert errors: {:?}", errors);
                anyhow::bail!("Failed to insert phash row: {:?}", errors);
            }
        }

        log::debug!("Successfully inserted phash for video_id: {}", video_id);

        // Also push to kvrocks
        let phash_data = VideohashPhash {
            video_id: video_id.to_string(),
            phash: phash.to_string(),
            num_frames: 10,
            hash_size: 8,
            duration: metadata.duration,
            width: metadata.width as i64,
            height: metadata.height as i64,
            fps: metadata.fps,
            created_at: chrono::Utc::now().to_rfc3339(),
        };
        if let Err(e) = kvrocks_client.store_videohash_phash(&phash_data).await {
            log::error!("Error pushing phash to kvrocks: {}", e);
        }

        Ok(())
    }

    async fn store_videohash_to_dedup_index(
        &self,
        agent: &ic_agent::Agent,
        video_id: &str,
        hash: &str,
    ) -> anyhow::Result<()> {
        let dedup_index = DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent);
        let now = SystemTime::now();

        let now = now.duration_since(UNIX_EPOCH).unwrap();
        dedup_index
            .add_video_to_index(
                video_id.into(),
                (
                    hash.into(),
                    CanisterSystemTime {
                        nanos_since_epoch: now.subsec_nanos(),
                        secs_since_epoch: now.as_secs(),
                    },
                ),
            )
            .await
            .context("Couldn't add video to dedup index")?;
        Ok(())
    }

    async fn store_unique_video_v2(
        &self,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        hash: &str,
    ) -> Result<(), anyhow::Error> {
        let bigquery_client = app_state::init_bigquery_client().await;

        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.video_unique_v2`
             (video_id, videohash, created_at)
             VALUES ('{video_id}', '{hash}', CURRENT_TIMESTAMP())"
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        log::info!("Storing unique video in video_unique_v2 for video_id [{video_id}]");

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        // Also push to kvrocks
        let unique_data = VideoUniqueV2 {
            video_id: video_id.to_string(),
            videohash: hash.to_string(),
            created_at: chrono::Utc::now().to_rfc3339(),
        };
        if let Err(e) = kvrocks_client.store_video_unique_v2(&unique_data).await {
            log::error!("Error pushing video_unique_v2 to kvrocks: {}", e);
        }

        Ok(())
    }

    async fn store_user_uploaded_content_approval(
        &self,
        bigquery_client: &google_cloud_bigquery::client::Client,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        post_id: &str,
        user_id: &str,
    ) -> Result<(), anyhow::Error> {
        log::info!("Processing content approval for video_id: {}", video_id);

        // Query both country and canister_id from the video_upload_successful event
        let event_query = format!(
            "SELECT JSON_EXTRACT_SCALAR(params, '$.country') AS country,
                    JSON_EXTRACT_SCALAR(params, '$.canister_id') AS canister_id
             FROM `hot-or-not-feed-intelligence.analytics_335143420.test_events_analytics`
             WHERE event = 'video_upload_successful'
               AND JSON_EXTRACT_SCALAR(params, '$.video_id') = '{}'
             LIMIT 1",
            video_id
        );

        let event_request = QueryRequest {
            query: event_query,
            ..Default::default()
        };

        let event_result = bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &event_request)
            .await;

        let (is_bot, canister_id) = match event_result {
            Ok(response) => {
                if let Some(rows) = response.rows {
                    if let Some(row) = rows.first() {
                        let is_bot = row.f.first().is_some_and(|cell| match &cell.v {
                            google_cloud_bigquery::http::tabledata::list::Value::String(
                                country,
                            ) => {
                                let is_bot = country.ends_with("-BOT");
                                log::debug!(
                                    "Video {} has country '{}', is_bot: {}",
                                    video_id,
                                    country,
                                    is_bot
                                );
                                is_bot
                            }
                            _ => false,
                        });

                        let canister_id = row.f.get(1).and_then(|cell| match &cell.v {
                            google_cloud_bigquery::http::tabledata::list::Value::String(id) => {
                                Some(id.clone())
                            }
                            _ => None,
                        });

                        (is_bot, canister_id)
                    } else {
                        log::debug!(
                            "No video_upload_successful event found for video {}",
                            video_id
                        );
                        (false, None)
                    }
                } else {
                    (false, None)
                }
            }
            Err(e) => {
                log::warn!("Failed to query event data for video {}: {}", video_id, e);
                (false, None)
            }
        };

        // Store video metadata for all videos
        let metadata = KvrocksVideoMetadata {
            video_id: video_id.to_string(),
            post_id: post_id.to_string(),
            publisher_user_id: user_id.to_string(),
        };
        if let Err(e) = kvrocks_client.store_video_metadata(&metadata).await {
            log::error!("Error storing video metadata to kvrocks: {}", e);
        }

        if is_bot {
            log::info!(
                "Bot video detected, storing bot marker for video: {}",
                video_id
            );
            // Store bot marker in kvrocks
            let bot_data = BotUploadedAiContent {
                video_id: video_id.to_string(),
                created_at: chrono::Utc::now().to_rfc3339(),
            };
            if let Err(e) = kvrocks_client
                .store_bot_uploaded_ai_content(&bot_data)
                .await
            {
                log::error!("Error storing bot marker to kvrocks: {}", e);
            }
            return Ok(());
        }

        // Use standard SQL INSERT instead of streaming insert
        let canister_id_sql = canister_id
            .as_ref()
            .map(|id| format!("'{}'", id.replace('\'', "''")))
            .unwrap_or_else(|| "NULL".to_string());

        let query = format!(
            "INSERT INTO `hot-or-not-feed-intelligence.yral_ds.ugc_content_approval`
             (video_id, post_id, canister_id, user_id, is_approved, created_at)
             VALUES ('{}', '{}', {}, '{}', FALSE, CURRENT_TIMESTAMP())",
            video_id.replace('\'', "''"),
            post_id.replace('\'', "''"),
            canister_id_sql,
            user_id.replace('\'', "''")
        );

        let request = QueryRequest {
            query,
            ..Default::default()
        };

        bigquery_client
            .job()
            .query("hot-or-not-feed-intelligence", &request)
            .await?;

        log::info!(
            "Successfully inserted ugc_content_approval: video_id={}, post_id={}, canister_id={:?}, user_id={}, is_approved=false",
            video_id,
            post_id,
            canister_id,
            user_id
        );

        // Also push to kvrocks
        let approval_data = UserUploadedContentApproval {
            video_id: video_id.to_string(),
            post_id: post_id.to_string(),
            canister_id: canister_id.unwrap_or_default(),
            user_id: user_id.to_string(),
            is_approved: false,
            created_at: chrono::Utc::now().to_rfc3339(),
        };
        if let Err(e) = kvrocks_client
            .store_user_uploaded_content_approval(&approval_data)
            .await
        {
            log::error!(
                "Error pushing user_uploaded_content_approval to kvrocks: {}",
                e
            );
        }

        Ok(())
    }

    /// Check Redis for exact phash match (tier-1 caching)
    async fn check_exact_duplicate_in_redis(
        redis_pool: &RedisPool,
        phash: &str,
    ) -> Result<Option<String>, anyhow::Error> {
        let mut conn = redis_pool
            .get()
            .await
            .context("Failed to get Redis connection")?;

        let key = format!("video_phash:{}", phash);
        let result: Option<String> = redis::cmd("GET")
            .arg(&key)
            .query_async(&mut *conn)
            .await
            .context("Failed to query Redis for phash")?;

        Ok(result)
    }

    /// Store unique phash in Redis for fast exact match lookups
    async fn store_unique_phash_in_redis(
        redis_pool: &RedisPool,
        phash: &str,
        video_id: &str,
    ) -> Result<(), anyhow::Error> {
        let mut conn = redis_pool
            .get()
            .await
            .context("Failed to get Redis connection")?;

        let key = format!("video_phash:{}", phash);
        redis::cmd("SET")
            .arg(&key)
            .arg(video_id)
            .query_async::<()>(&mut *conn)
            .await
            .context("Failed to store phash in Redis")?;

        Ok(())
    }

    /// V2 version that uses Milvus for deduplication instead of DedupIndex canister
    /// Provides configurable hamming distance threshold, Redis tier-1 caching, and graceful fallback
    #[cfg(not(feature = "local-bin"))]
    #[allow(clippy::too_many_arguments)]
    pub async fn process_video_deduplication_v2<'a>(
        &self,
        agent: &ic_agent::Agent,
        bigquery_client: &google_cloud_bigquery::client::Client,
        milvus_client: &Option<crate::milvus::Client>,
        redis_pool: &RedisPool,
        kvrocks_client: &KvrocksClient,
        video_id: &str,
        _video_url: &str,
        publisher_data: VideoPublisherDataV2,
        hamming_threshold: u32,
        publish_video_callback: impl FnOnce(
            &str,
            String,
            String,
            &str,
        )
            -> futures::future::BoxFuture<'a, Result<(), anyhow::Error>>,
    ) -> Result<(), anyhow::Error> {
        log::info!(
            "Computing phash for video ID: {video_id} (v2 with Milvus, threshold={})",
            hamming_threshold
        );
        let (phash, metadata) =
            compute_phash_from_storj(&publisher_data.publisher_principal, video_id)
                .await
                .map_err(|e| anyhow::anyhow!("Failed to compute phash: {}", e))?;

        // TIER 1: Check Redis for exact match (FAST - <1ms)
        log::debug!("Tier 1: Checking Redis for exact phash match");
        if let Ok(Some(existing_video_id)) =
            Self::check_exact_duplicate_in_redis(redis_pool, &phash).await
        {
            log::info!(
                "âš¡ EXACT DUPLICATE (Redis): Video {} has identical phash to {}",
                video_id,
                existing_video_id
            );

            // Store metadata (kvrocks push is inside the functions)
            self.store_videohash_original(bigquery_client, kvrocks_client, video_id, &phash)
                .await?;
            self.store_phash_to_bigquery(
                bigquery_client,
                kvrocks_client,
                video_id,
                &phash,
                &metadata,
            )
            .await?;
            self.store_user_uploaded_content_approval(
                bigquery_client,
                kvrocks_client,
                video_id,
                &publisher_data.post_id,
                &publisher_data.publisher_principal,
            )
            .await?;

            // Continue with video processing pipeline
            let timestamp = chrono::Utc::now().to_rfc3339();
            publish_video_callback(
                video_id,
                publisher_data.post_id,
                timestamp,
                &publisher_data.publisher_principal,
            )
            .await?;

            return Ok(());
        }

        log::debug!("Tier 1: No exact match in Redis");

        // TIER 2: Check Milvus for similar matches (SLOWER - 10-50ms)
        log::debug!(
            "Tier 2: Checking Milvus for similar videos (Hamming distance < {})",
            hamming_threshold
        );
        let is_duplicate = if let Some(client) = milvus_client {
            log::debug!(
                "Checking Milvus for duplicates with threshold {}",
                hamming_threshold
            );
            match crate::milvus::search_similar_videos(client, &phash, hamming_threshold).await {
                Ok(results) => {
                    let is_dup = !results.is_empty();
                    if is_dup {
                        log::info!(
                            "ðŸ” SIMILAR DUPLICATE (Milvus): Video {} matches {} videos",
                            video_id,
                            results.len()
                        );
                    }
                    is_dup
                }
                Err(e) => {
                    log::warn!(
                        "Milvus search failed ({}), falling back to DedupIndex: {}",
                        video_id,
                        e
                    );
                    // Fallback to DedupIndex canister
                    DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent)
                        .is_duplicate(phash.clone())
                        .await
                        .context("Couldn't check if the video is duplicate (fallback)")?
                }
            }
        } else {
            log::debug!("Milvus client not available, using DedupIndex canister");
            // No Milvus client, use DedupIndex canister
            DedupIndex(*DEDUP_INDEX_CANISTER_ID, agent)
                .is_duplicate(phash.clone())
                .await
                .context("Couldn't check if the video is duplicate")?
        };

        // Store the phash regardless of duplication status (kvrocks push is inside the functions)
        self.store_videohash_original(bigquery_client, kvrocks_client, video_id, &phash)
            .await?;
        self.store_phash_to_bigquery(bigquery_client, kvrocks_client, video_id, &phash, &metadata)
            .await?;
        self.store_user_uploaded_content_approval(
            bigquery_client,
            kvrocks_client,
            video_id,
            &publisher_data.post_id,
            &publisher_data.publisher_principal,
        )
        .await?;

        if !is_duplicate {
            log::info!("âœ¨ UNIQUE: Video {} has a unique phash", video_id);

            // Insert into Milvus + Redis (only for unique videos)
            if let Some(client) = milvus_client {
                let created_at = chrono::Utc::now().timestamp();

                // Insert into Milvus
                log::debug!("Inserting unique video into Milvus: {}", video_id);
                if let Err(e) =
                    crate::milvus::insert_video_hash(client, video_id, &phash, created_at).await
                {
                    log::error!("Failed to insert video {} into Milvus: {}", video_id, e);
                }

                // Insert into Redis for fast exact match caching
                if let Err(e) =
                    Self::store_unique_phash_in_redis(redis_pool, &phash, video_id).await
                {
                    log::error!("Failed to insert video {} into Redis: {}", video_id, e);
                }
            }

            self.store_unique_video_v2(kvrocks_client, video_id, &phash)
                .await?;
            log::info!("Unique video recorded in BigQuery: video_id [{video_id}]");
        } else {
            log::debug!(
                "Video {} is a duplicate, NOT storing in Milvus/Redis",
                video_id
            );
        }

        // Always proceed with normal video processing, regardless of duplicate status
        let timestamp = chrono::Utc::now().to_rfc3339();
        publish_video_callback(
            video_id,
            publisher_data.post_id,
            timestamp,
            &publisher_data.publisher_principal,
        )
        .await?;

        Ok(())
    }
}
